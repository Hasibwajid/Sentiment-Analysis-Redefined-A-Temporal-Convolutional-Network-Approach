{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading dataset from kaggle using kaggle data api"
      ],
      "metadata": {
        "id": "7Cnn2lSyjPPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to kaggle for dataset downlord\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "\n",
        "#downlord dataset\n",
        "!kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
        "\n",
        "#unzip dataset\n",
        "!unzip imdb-dataset-of-50k-movie-reviews.zip\n",
        "\n",
        "#reading only top SOME SAMPLE rows\n",
        "import pandas as pd\n",
        "df = pd.read_csv('IMDB Dataset.csv').head(50000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5dpCtKVf-HM",
        "outputId": "fb8d0f28-0bc8-4f20-d440-e2d8d9547533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading imdb-dataset-of-50k-movie-reviews.zip to /content\n",
            " 97% 25.0M/25.7M [00:00<00:00, 82.5MB/s]\n",
            "100% 25.7M/25.7M [00:00<00:00, 88.3MB/s]\n",
            "Archive:  imdb-dataset-of-50k-movie-reviews.zip\n",
            "  inflating: IMDB Dataset.csv        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing dataset"
      ],
      "metadata": {
        "id": "ENeDa3C0jb5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "X = df['review'].values\n",
        "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(X).toarray()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "# Define constants\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 128\n",
        "output_size = 2\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "kjK8GeO3f_X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture"
      ],
      "metadata": {
        "id": "BMFmL5ZNjgoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While models like RNNs and LSTMs have been widely used for sequence modeling tasks, TCNs present an alternative that combines efficiency, parallelization, and the ability to capture long-range dependencies, making them suitable for tasks like sentiment analysis where contextual information matters."
      ],
      "metadata": {
        "id": "1Lk3Sc9dHnqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class SentimentTCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n",
        "        super(SentimentTCN, self).__init__()\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
        "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.tcn(x.unsqueeze(2))  # Adjust input dimensions\n",
        "        y = y[:, :, -1]  # Take the output at the last time step\n",
        "        return self.linear(y)"
      ],
      "metadata": {
        "id": "LqG9M6fygUeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluation"
      ],
      "metadata": {
        "id": "QZ8lIjEVjqy8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_size)\n",
        "# Instantiate the model\n",
        "model = SentimentTCN(input_size, output_size, num_channels=[hidden_size]*4, kernel_size=2, dropout=0.2)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = correct / total\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {val_loss/len(val_loader)}, Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVv-f9uff3J0",
        "outputId": "c950dc21-dcb1-4e3d-c1c5-89b9d2111ad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.28411131565737874, Accuracy: 0.8833\n",
            "Epoch 2/10, Loss: 0.2813898280357859, Accuracy: 0.8869\n",
            "Epoch 3/10, Loss: 0.27964116666157535, Accuracy: 0.8927\n",
            "Epoch 4/10, Loss: 0.3084000036310238, Accuracy: 0.8875\n",
            "Epoch 5/10, Loss: 0.3635955649386546, Accuracy: 0.8845\n",
            "Epoch 6/10, Loss: 0.44176918913604346, Accuracy: 0.8848\n",
            "Epoch 7/10, Loss: 0.49341675079172587, Accuracy: 0.8823\n",
            "Epoch 8/10, Loss: 0.49301986961038247, Accuracy: 0.8804\n",
            "Epoch 9/10, Loss: 0.5062911537991968, Accuracy: 0.8833\n",
            "Epoch 10/10, Loss: 0.5443131752834198, Accuracy: 0.8794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4532HUX0Rvf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}